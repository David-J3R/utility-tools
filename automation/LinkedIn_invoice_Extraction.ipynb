{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GRFGHztk3i8"
      },
      "outputs": [],
      "source": [
        "!pip install pdfplumber\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tabula-py"
      ],
      "metadata": {
        "id": "5sx5JGnmmXff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LinkedIn Invoice PDF Table Extraction"
      ],
      "metadata": {
        "id": "An4_ajXcivS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber # Library for extraction from PDF files\n",
        "import pandas as pd\n",
        "import re\n",
        "import os"
      ],
      "metadata": {
        "id": "eBGvmlpti0N1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goal**: To extract line-item data from LinkedIn PDF invoice and sum up campaigns per region, and exported as an Excel file\n",
        "\n",
        "This code has been assisted by AI [Claude 3.7 Sonnet & Gemini 2.5 pro preview 05-06]\n",
        "\n",
        "Disclaimer: There are some unnecessary lines of code for verification processes that I decided to leave to use as references for future projects\n",
        "\n",
        "**Important**: the code consider the next specific formats:\n",
        "- Campaign name format: \"region_channel_name_date\"\n",
        "- A LinkedIn invoice of 5 pages (If the invoice is larger, you should modify the \"if\" condition in line 47 of the Main Script)"
      ],
      "metadata": {
        "id": "8oQO6BAJi1uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PDF_FILE_PATH = \"SampleInvoice_LinkedIn.pdf\""
      ],
      "metadata": {
        "id": "e3Hptk7sk6LL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "WgrOeifhlshd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper Functions (mostly unchanged) ---\n",
        "# Extract the region from the campaign name\n",
        "def extract_region(desc_text):\n",
        "    if not isinstance(desc_text, str) or not desc_text.strip(): # Is actually a string\n",
        "        return 'unknown'\n",
        "    # This is because the \"Description\" cell can have multiple lines\n",
        "    first_line = desc_text.split('\\n')[0].strip()\n",
        "    # Campaign name format: \"region_channel_name_date\"\n",
        "    match = re.search(r'Campaign:\\s*([a-zA-Z0-9_]+)', first_line, re.IGNORECASE) # Match object\n",
        "    if match:\n",
        "        full_campaign_tag = match.group(1)\n",
        "        region = full_campaign_tag.split('_')[0]\n",
        "        return region.lower()\n",
        "    return 'unknown'\n",
        "\n",
        "# Convert monetary amount into float\n",
        "def clean_billed_amount(amount_text):\n",
        "    if pd.isna(amount_text):\n",
        "        return 0.0\n",
        "    if isinstance(amount_text, (int, float)):\n",
        "        return float(amount_text)\n",
        "    if isinstance(amount_text, str):\n",
        "        try:\n",
        "            return float(amount_text.replace(',', '').replace('\\n', '').strip())\n",
        "        except ValueError:\n",
        "            print(f\"Warning: Could not convert '{amount_text}' to float.\")\n",
        "            return 0.0\n",
        "    return 0.0\n",
        "\n",
        "# Find a desired column name within a list of extracted column names\n",
        "# NOTE: Only useful when the header names are not defined\n",
        "def find_column_name(available_cols, target_keywords, exact_match_first=None):\n",
        "    #Robustly finds a column name\n",
        "    if not available_cols: return None\n",
        "\n",
        "    cleaned_available_cols = [str(col).replace('\\n', ' ').strip() for col in available_cols if col]\n",
        "    # specific name to prioritize if found exactly\n",
        "    if exact_match_first:\n",
        "        for col_name in cleaned_available_cols:\n",
        "            if exact_match_first.lower() == col_name.lower():\n",
        "                return col_name # Return the cleaned version that matched\n",
        "\n",
        "    for keyword in target_keywords:\n",
        "        for col_name in cleaned_available_cols:\n",
        "            if keyword.lower() in col_name.lower():\n",
        "                return col_name # Return the cleaned version that matched\n",
        "    return None"
      ],
      "metadata": {
        "id": "GJ7kNWbzlZDr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Script"
      ],
      "metadata": {
        "id": "T-E720yu0JZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Main Script --\n",
        "if not os.path.exists(PDF_FILE_PATH):\n",
        "    print(f\"Error: PDF file not found at '{PDF_FILE_PATH}'\")\n",
        "    exit()\n",
        "\n",
        "all_extracted_dataframes = []\n",
        "# List of header keywords expected\n",
        "TABLE_HEADER_KEYWORDS = [\"Line\", \"Description\", \"Qty\", \"Unit Price\", \"Billed Amount\"] # Key parts of the target table header\n",
        "\n",
        "print(f\"Opening PDF: {PDF_FILE_PATH}\")\n",
        "\n",
        "try:\n",
        "    with pdfplumber.open(PDF_FILE_PATH) as pdf:\n",
        "        pages_to_process = pdf.pages[:-1] # Exclude the last LinkedIn Invoice summary page\n",
        "\n",
        "        for i, page in enumerate(pages_to_process):\n",
        "            page_num_for_display = i + 1\n",
        "            print(f\"--- Processing Page {page_num_for_display}/{len(pages_to_process)} ---\")\n",
        "\n",
        "            table_data = None\n",
        "            crop_box = None\n",
        "\n",
        "            ## 1. Find the top of the actual table by looking for its header ##\n",
        "            header_line_top = None\n",
        "            # Using extract_words for more granular control if needed, but lines is often good.\n",
        "            # Each item in text_lines is a dictionary containing information about the line\n",
        "            text_lines = page.extract_text_lines(layout=True, strip=True, return_chars=False)\n",
        "            if not text_lines: text_lines = [] # Ensure it's iterable\n",
        "\n",
        "            for line_info in text_lines:\n",
        "                line_text_lower = line_info[\"text\"].lower()\n",
        "                # Check if a good number of table header keywords are in this line\n",
        "                if sum(keyword.lower() in line_text_lower for keyword in TABLE_HEADER_KEYWORDS) >= 3: # Heuristic\n",
        "                    header_line_top = line_info[\"top\"] # Stores y-coordinate of this presumed header line\n",
        "                    print(f\"  Found potential table header line: '{line_info['text']}' at top: {header_line_top}\")\n",
        "                    break\n",
        "\n",
        "            if header_line_top:\n",
        "              # *PDF COORDINATES: in pdfplumber, the origin [0,0] is at the bottom-left of the page.\n",
        "                # 2. Define crop box\n",
        "                crop_y_start = header_line_top - 10 # Start slightly above the detected header\n",
        "                crop_y_end = page.height * 0.92    # Default end, before typical footers\n",
        "\n",
        "                ## -- NOTE:  adjust this part depending on how many pages the LinkedIn invoice has\n",
        "                ## and where the table ends\n",
        "                # Adjust crop_y_end for page 5 specifically\n",
        "                if page_num_for_display == 5:\n",
        "                    temp_crop_y_end_page5 = crop_y_end # Start with default\n",
        "                    for line_info in text_lines:\n",
        "                        line_text_lower = line_info[\"text\"].lower()\n",
        "                        # Look for markers indicating end of table data on page 5\n",
        "                        # \"special instructions\" the end of the table\n",
        "                        if \"special instructions\" in line_text_lower or \\\n",
        "                           (\"total\" in line_text_lower and line_info[\"x0\"] > page.width * 0.4): # \"Total\" on right side\n",
        "                            # If this marker is above our current crop_y_start\n",
        "                            if line_info[\"top\"] > crop_y_start: # Ensure this end-of-table marker is below the starting point\n",
        "                                temp_crop_y_end_page5 = min(temp_crop_y_end_page5, line_info[\"top\"] - 5)\n",
        "                    if temp_crop_y_end_page5 < crop_y_end : # If we found a better end\n",
        "                        crop_y_end = temp_crop_y_end_page5\n",
        "                        print(f\"  Page 5: Adjusted crop_y_end to {crop_y_end} based on content.\")\n",
        "\n",
        "                # For a valid crop crop_y_end should be less than crop_y_start\n",
        "                if crop_y_start < crop_y_end :\n",
        "                  # pdfplumber's .crop() method expects (x0, top, x1, bottom)\n",
        "                    crop_box = (page.bbox[0] + 10, crop_y_start, page.bbox[2] - 10, crop_y_end) # Slight L/R margin\n",
        "                    print(f\"  Applying crop_box: {crop_box}\")\n",
        "\n",
        "                    try:\n",
        "                        cropped_page = page.crop(crop_box)\n",
        "\n",
        "                        # 3. Extract table from the cropped page\n",
        "                        # Try text-based strategy first on the cleaner, cropped area\n",
        "                        crop_table_settings_text = {\n",
        "                            \"vertical_strategy\": \"text\", \"horizontal_strategy\": \"text\",\n",
        "                            \"text_x_tolerance\": 3, \"text_y_tolerance\": 3,\n",
        "                            \"snap_tolerance\": 3, \"join_tolerance\": 3,\n",
        "                        }\n",
        "                        table_data = cropped_page.extract_table(table_settings=crop_table_settings_text)\n",
        "\n",
        "                        # If text strategy fails or yields little, try lines strategy (as table has some lines)\n",
        "                        if not table_data or len(table_data) < 2:\n",
        "                            print(f\"  Text strategy on cropped page yielded minimal data. Trying lines strategy...\")\n",
        "                            crop_table_settings_lines = {\n",
        "                                \"vertical_strategy\": \"lines\", \"horizontal_strategy\": \"lines\",\n",
        "                                \"snap_tolerance\": 3, \"join_tolerance\": 3,\n",
        "                            }\n",
        "                            table_data = cropped_page.extract_table(table_settings=crop_table_settings_lines)\n",
        "                    except Exception as crop_ex:\n",
        "                        print(f\"  Error during cropping or extracting from cropped page: {crop_ex}\")\n",
        "                        table_data = None # Ensure table_data is None if crop fails\n",
        "                else:\n",
        "                    print(f\"  Invalid crop box (start_y >= end_y) for page {page_num_for_display}. Crop_y_start: {crop_y_start}, Crop_y_end: {crop_y_end}. Skipping crop.\")\n",
        "            else:\n",
        "                print(f\"  Could not find table header keywords to define crop_top on page {page_num_for_display}.\")\n",
        "\n",
        "\n",
        "            # 4. Validate and process extracted table_data\n",
        "            if table_data and len(table_data) >= 1:\n",
        "                raw_header = table_data[0]\n",
        "                data_rows = table_data[1:]\n",
        "                cleaned_header = [str(col).replace('\\n', ' ').strip() if col is not None else f\"Unnamed_{j}\" for j, col in enumerate(raw_header)]\n",
        "\n",
        "                print(f\"  Extracted with Cleaned Header: {cleaned_header}\")\n",
        "\n",
        "                # Validate if this header looks like our target table's header\n",
        "                header_match_count = sum(keyword.lower() in str(ch).lower() for ch in cleaned_header for keyword in TABLE_HEADER_KEYWORDS)\n",
        "\n",
        "                if header_match_count >= 3 and len(data_rows) > 0: # Expect at least 3 keyword matches\n",
        "                    df_page = pd.DataFrame(data_rows, columns=cleaned_header)\n",
        "                    all_extracted_dataframes.append(df_page)\n",
        "                    print(f\"  SUCCESS: Valid table extracted and added from page {page_num_for_display}.\")\n",
        "\n",
        "                else:\n",
        "                    print(f\"  WARNING: Extracted table header (match_count: {header_match_count}) does not sufficiently match target. Discarding.\")\n",
        "                    # For debugging:\n",
        "                    # if 'cropped_page' in locals() and cropped_page:\n",
        "                    #     img = cropped_page.to_image(resolution=150)\n",
        "                    #     img.save(f\"debug_discarded_crop_page_{page_num_for_display}.png\")\n",
        "                    #     print(f\"  Saved debug_discarded_crop_page_{page_num_for_display}.png\")\n",
        "            else:\n",
        "                print(f\"  No valid table data extracted from page {page_num_for_display} with current methods.\")\n",
        "                # If crop was attempted but failed, or no header found, save an image of the full page for review\n",
        "                # img = page.to_image(resolution=150)\n",
        "                # img.save(f\"debug_full_page_failed_extraction_{page_num_for_display}.png\")\n",
        "                # print(f\"  Saved debug_full_page_failed_extraction_{page_num_for_display}.png for review.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during PDF processing loop: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "bQa9btlb0K5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc36d623-6d9c-4887-8a2b-d85100665a1c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening PDF: SampleInvoice_LinkedIn.pdf\n",
            "--- Processing Page 1/5 ---\n",
            "  Found potential table header line: 'Line Description                        Qty    Unit Price Billed Amount VAT Amount' at top: 266.5621609450001\n",
            "  Applying crop_box: (10, 256.5621609450001, 602, 728.6400561522)\n",
            "  Extracted with Cleaned Header: ['Line', 'Description', 'Qty Unit', 'Price Billed', 'Amount VAT', 'Amount']\n",
            "  SUCCESS: Valid table extracted and added from page 1.\n",
            "--- Processing Page 2/5 ---\n",
            "  Found potential table header line: 'Line Description                        Qty    Unit Price Billed Amount VAT Amount' at top: 123.762160945\n",
            "  Applying crop_box: (10, 113.762160945, 602, 728.6400561522)\n",
            "  Extracted with Cleaned Header: ['Line', 'Description', 'Qty Unit', 'Price Billed', 'Amount VAT', 'Amount']\n",
            "  SUCCESS: Valid table extracted and added from page 2.\n",
            "--- Processing Page 3/5 ---\n",
            "  Found potential table header line: 'Line Description                        Qty    Unit Price Billed Amount VAT Amount' at top: 123.762160945\n",
            "  Applying crop_box: (10, 113.762160945, 602, 728.6400561522)\n",
            "  Extracted with Cleaned Header: ['Line', 'Description', 'Qty Unit', 'Price Billed', 'Amount VAT', 'Amount']\n",
            "  SUCCESS: Valid table extracted and added from page 3.\n",
            "--- Processing Page 4/5 ---\n",
            "  Found potential table header line: 'Line Description                        Qty    Unit Price Billed Amount VAT Amount' at top: 123.762160945\n",
            "  Applying crop_box: (10, 113.762160945, 602, 728.6400561522)\n",
            "  Extracted with Cleaned Header: ['Line', 'Description', 'Qty Unit', 'Price Billed', 'Amount VAT', 'Amount']\n",
            "  SUCCESS: Valid table extracted and added from page 4.\n",
            "--- Processing Page 5/5 ---\n",
            "  Found potential table header line: 'Line Description                        Qty    Unit Price Billed Amount VAT Amount' at top: 123.762160945\n",
            "  Page 5: Adjusted crop_y_end to 260.5616403350001 based on content.\n",
            "  Applying crop_box: (10, 113.762160945, 602, 260.5616403350001)\n",
            "  Extracted with Cleaned Header: ['Line', 'Description', 'Qty Unit', 'Price Billed', 'Amount VAT', 'Amount']\n",
            "  SUCCESS: Valid table extracted and added from page 5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- This section will now only proceed if tables were actually extracted ---\n",
        "if not all_extracted_dataframes:\n",
        "    print(\"\\nCRITICAL ERROR: No tables were successfully extracted from the PDF after all attempts.\")\n",
        "    print(\"This means the cropping and table extraction logic could not identify and parse the target table.\")\n",
        "    print(\"Suggestions for debugging:\")\n",
        "    print(\"1. Uncomment the `.to_image().save(...)` lines in the loop to visually inspect what is being cropped and what `pdfplumber` 'sees'.\")\n",
        "    print(\"2. Check the `TABLE_HEADER_KEYWORDS` and the logic for finding `header_line_top` (e.g., the `sum(...) >= 3` heuristic).\")\n",
        "    print(\"3. Fine-tune the `crop_y_start` and `crop_y_end` calculations, especially the bottom boundary for page 5.\")\n",
        "    print(\"4. Experiment with different `crop_table_settings_*` on the `cropped_page`.\")\n",
        "    print(\"5. Ensure `pdfplumber` is up-to-date: `pip install --upgrade pdfplumber`.\")\n",
        "    exit() # Explicitly exit to prevent the pd.concat error\n",
        "\n",
        "# Combine all successfully extracted DataFrames\n",
        "df_combined = pd.concat(all_extracted_dataframes, ignore_index=True)\n",
        "\n",
        "print(\"\\n--- Combined DataFrame (First 5 rows before processing) ---\")\n",
        "print(df_combined.head())\n",
        "print(\"\\n--- Combined DataFrame Info ---\")\n",
        "df_combined.info()\n",
        "## Renaming headers correctly\n",
        "df_combined.columns = ['Line', 'Description', 'Qty', 'Unit Price', 'Billed Amount', 'VAT Amount']\n",
        "print(\"\\n--- Updated Column Names ---\")\n",
        "print(df_combined.columns)\n",
        "print(\"\\n--- Combined DataFrame Columns ---\")\n",
        "extracted_cols = df_combined.columns.tolist()\n",
        "print(f\"Original extracted columns: {extracted_cols}\")\n",
        "\n",
        "\n",
        "#NOTE: Only use if the column names change or there are variations with original names\n",
        "# Dynamically find the actual column names for 'Description' and 'Billed Amount'\n",
        "# using the cleaned column names we would have created during table processing\n",
        "#actual_description_col_name = find_column_name(extracted_cols, [\"description\"], exact_match_first=\"Description\")\n",
        "#actual_billed_amount_col_name = find_column_name(extracted_cols, [\"billed amount\", \"amount\"], exact_match_first=\"Billed Amount\")\n",
        "\n",
        "# Using Col names\n",
        "actual_description_col_name = \"Description\"\n",
        "actual_billed_amount_col_name = \"Billed Amount\"\n",
        "\n",
        "# Merge multi-line descriptions\n",
        "print(\"\\n--- Merging multi-line descriptions ---\")\n",
        "actual_line_col_name = \"Line\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Note: Leaving the If condition only for future references\n",
        "# Fallback to positional guessing if keyword search fails (less ideal)\n",
        "if not actual_description_col_name and len(extracted_cols) > 1:\n",
        "    actual_description_col_name = extracted_cols[1] # Assuming Description is often 2nd col\n",
        "    print(f\"  Warning: 'Description' column identified by position: {actual_description_col_name}\")\n",
        "if not actual_billed_amount_col_name and len(extracted_cols) > 4:\n",
        "    actual_billed_amount_col_name = extracted_cols[4] # Assuming Billed Amount is often 5th col\n",
        "    print(f\"  Warning: 'Billed Amount' column identified by position: {actual_billed_amount_col_name}\")\n",
        "\n",
        "if not actual_description_col_name:\n",
        "    print(f\"\\nFATAL ERROR: 'Description' column could not be identified. Aborting. Available: {extracted_cols}\")\n",
        "    exit()\n",
        "if not actual_billed_amount_col_name:\n",
        "    print(f\"\\nFATAL ERROR: 'Billed Amount' column could not be identified. Aborting. Available: {extracted_cols}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "print(f\"\\nUsing column '{actual_description_col_name}' for descriptions.\")\n",
        "print(f\"Using column '{actual_billed_amount_col_name}' for billed amounts.\")\n",
        "print(f\"Using column '{actual_line_col_name}' as the Line item identifier.\")\n",
        "\n",
        "# Ensure description and line columns exist\n",
        "if actual_description_col_name not in df_combined.columns or actual_line_col_name not in df_combined.columns:\n",
        "    print(\"Error: 'Description' or 'Line' column not found in df_combined. Skipping description merge.\")\n",
        "else:\n",
        "    processed_rows = []\n",
        "    current_description_parts = []\n",
        "    current_line_item_data = None\n",
        "\n",
        "    for index, row in df_combined.iterrows():\n",
        "        line_value = row[actual_line_col_name]\n",
        "        description_text = str(row[actual_description_col_name] or '').strip() # Handle None and strip\n",
        "\n",
        "        # Check if line_value is not NaN, not None, and not an empty string\n",
        "        is_new_line_item = pd.notna(line_value) and str(line_value).strip() != \"\"\n",
        "\n",
        "        if is_new_line_item:\n",
        "            # If there was a previous item being built, finalize and add it\n",
        "            if current_line_item_data is not None:\n",
        "                # Join collected description parts, ensuring no leading/trailing newlines from empty parts\n",
        "                full_description = \"\\n\".join(filter(None, current_description_parts)) # filter(None,...) removes empty strings\n",
        "                current_line_item_data[actual_description_col_name] = full_description\n",
        "                processed_rows.append(current_line_item_data)\n",
        "\n",
        "            # Start a new item\n",
        "            current_line_item_data = row.to_dict() # Store the entire row data for the main line item\n",
        "            current_description_parts = [description_text] if description_text else [] # Start new list, only if text exists\n",
        "        else:\n",
        "            # This is a continuation of the description for the current_line_item\n",
        "            if current_line_item_data is not None: # Only append if we are actively tracking an item\n",
        "                if description_text: # Only append if there's actual text\n",
        "                    current_description_parts.append(description_text)\n",
        "            # else:\n",
        "                # This is a row without a line number and no active item,\n",
        "                # could be an orphaned description part.\n",
        "                # Depending on requirements, you might want to handle this (e.g., log it or try to assign it)\n",
        "                # For now, it will be ignored if it doesn't follow a line item.\n",
        "\n",
        "    # Add the last processed item after the loop finishes\n",
        "    if current_line_item_data is not None:\n",
        "        full_description = \"\\n\".join(filter(None, current_description_parts))\n",
        "        current_line_item_data[actual_description_col_name] = full_description\n",
        "        processed_rows.append(current_line_item_data)\n",
        "\n",
        "    if processed_rows:\n",
        "        df_combined = pd.DataFrame(processed_rows)\n",
        "        print(\"  SUCCESS: Multi-line descriptions merged.\")\n",
        "        print(\"\\n--- Combined DataFrame (First 5 rows after description merge) ---\")\n",
        "        print(df_combined.head())\n",
        "        print(\"\\n--- Combined DataFrame Info (After description merge) ---\")\n",
        "        df_combined.info()\n",
        "    else:\n",
        "        print(\"  WARNING: No rows were processed during description merge. df_combined might be empty or incorrectly structured.\")\n",
        "\n",
        "\n",
        "# --- Data Processing on the combined DataFrame ---\n",
        "df_combined['Region'] = df_combined[actual_description_col_name].apply(extract_region)\n",
        "df_combined['Processed Billed Amount'] = df_combined[actual_billed_amount_col_name].apply(clean_billed_amount)\n",
        "\n",
        "print(\"\\n--- DataFrame with 'Region' and 'Processed Billed Amount' (First 10 rows) ---\")\n",
        "# Ensure the columns exist before trying to display them\n",
        "cols_to_display = []\n",
        "if actual_description_col_name in df_combined.columns: cols_to_display.append(actual_description_col_name)\n",
        "if 'Region' in df_combined.columns: cols_to_display.append('Region')\n",
        "if actual_billed_amount_col_name in df_combined.columns: cols_to_display.append(actual_billed_amount_col_name)\n",
        "if 'Processed Billed Amount' in df_combined.columns: cols_to_display.append('Processed Billed Amount')\n",
        "\n",
        "if cols_to_display:\n",
        "    print(df_combined[cols_to_display].head(10))\n",
        "else:\n",
        "    print(\"Could not display processed columns as key columns were not found.\")\n",
        "\n",
        "\n",
        "# Sum by region\n",
        "# At this point df_combined should have Region and Billed Amount columns\n",
        "# Just an extra verification\n",
        "if 'Region' in df_combined.columns and 'Processed Billed Amount' in df_combined.columns:\n",
        "    region_totals = df_combined.groupby('Region')['Processed Billed Amount'].sum().reset_index()\n",
        "    region_totals = region_totals.sort_values(by='Processed Billed Amount', ascending=False)\n",
        "    print(\"\\n--- Totals by Region ---\")\n",
        "    print(region_totals)\n",
        "else:\n",
        "    print(\"\\nCould not calculate totals by region due to missing 'Region' or 'Processed Billed Amount' columns.\")\n",
        "\n",
        "print(\"\\n--- Script Finished ---\")\n",
        "\n",
        "# Export as Excel file\n",
        "df_combined.to_excel(\"LinkedIn_Invoice_table2.xlsx\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7oR651Pu3ng",
        "outputId": "d1539d27-27dc-4960-b5a9-2c346920f9ba"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Combined DataFrame (First 5 rows before processing) ---\n",
            "  Line                               Description Qty Unit Price Billed  \\\n",
            "0                                                                        \n",
            "1    1  Campaign: americas_paid-social_document-   4665.4            1   \n",
            "2                                                                        \n",
            "3                       management_gartner-mq-24                         \n",
            "4                                                                        \n",
            "\n",
            "  Amount VAT Amount  \n",
            "0                    \n",
            "1   4,665.40   0.00  \n",
            "2                    \n",
            "3             0.00%  \n",
            "4                    \n",
            "\n",
            "--- Combined DataFrame Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 434 entries, 0 to 433\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   Line          434 non-null    object\n",
            " 1   Description   434 non-null    object\n",
            " 2   Qty Unit      434 non-null    object\n",
            " 3   Price Billed  434 non-null    object\n",
            " 4   Amount VAT    434 non-null    object\n",
            " 5   Amount        434 non-null    object\n",
            "dtypes: object(6)\n",
            "memory usage: 20.5+ KB\n",
            "\n",
            "--- Updated Column Names ---\n",
            "Index(['Line', 'Description', 'Qty', 'Unit Price', 'Billed Amount',\n",
            "       'VAT Amount'],\n",
            "      dtype='object')\n",
            "\n",
            "--- Combined DataFrame Columns ---\n",
            "Original extracted columns: ['Line', 'Description', 'Qty', 'Unit Price', 'Billed Amount', 'VAT Amount']\n",
            "\n",
            "--- Merging multi-line descriptions ---\n",
            "\n",
            "Using column 'Description' for descriptions.\n",
            "Using column 'Billed Amount' for billed amounts.\n",
            "Using column 'Line' as the Line item identifier.\n",
            "  SUCCESS: Multi-line descriptions merged.\n",
            "\n",
            "--- Combined DataFrame (First 5 rows after description merge) ---\n",
            "  Line                                        Description      Qty Unit Price  \\\n",
            "0    1  Campaign: americas_paid-social_document-\\nmana...   4665.4          1   \n",
            "1    2  Campaign: americas_paid-social_ecm_intent-doxi...   482.78          1   \n",
            "2    3  Campaign: americas_paid-social_ecm_intent-tei-...   762.92          1   \n",
            "3    4  Campaign: americas_paid-social_sap-\\nsuccessfa...  3125.01          1   \n",
            "4    5  Campaign: americas_paid-social_sap-\\nsuccessfa...      840          1   \n",
            "\n",
            "  Billed Amount VAT Amount  \n",
            "0      4,665.40       0.00  \n",
            "1        482.78       0.00  \n",
            "2        762.92       0.00  \n",
            "3      3,125.01       0.00  \n",
            "4        840.00       0.00  \n",
            "\n",
            "--- Combined DataFrame Info (After description merge) ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 44 entries, 0 to 43\n",
            "Data columns (total 6 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   Line           44 non-null     object\n",
            " 1   Description    44 non-null     object\n",
            " 2   Qty            44 non-null     object\n",
            " 3   Unit Price     44 non-null     object\n",
            " 4   Billed Amount  44 non-null     object\n",
            " 5   VAT Amount     44 non-null     object\n",
            "dtypes: object(6)\n",
            "memory usage: 2.2+ KB\n",
            "\n",
            "--- DataFrame with 'Region' and 'Processed Billed Amount' (First 10 rows) ---\n",
            "                                         Description    Region Billed Amount  \\\n",
            "0  Campaign: americas_paid-social_document-\\nmana...  americas      4,665.40   \n",
            "1  Campaign: americas_paid-social_ecm_intent-doxi...  americas        482.78   \n",
            "2  Campaign: americas_paid-social_ecm_intent-tei-...  americas        762.92   \n",
            "3  Campaign: americas_paid-social_sap-\\nsuccessfa...  americas      3,125.01   \n",
            "4  Campaign: americas_paid-social_sap-\\nsuccessfa...  americas        840.00   \n",
            "5  Campaign: americas_paid-social_sap-\\nsuccessfa...  americas      1,351.89   \n",
            "6  Campaign: americas_paid-social_sap-\\nsuccessfa...  americas        694.19   \n",
            "7  Campaign: dach_paid-social_brand_tei-carousel\\...      dach         33.00   \n",
            "8  Campaign: dach_paid-social_document-\\nmanageme...      dach      2,468.06   \n",
            "9  Campaign: dach_paid-social_document-\\nmanageme...      dach        109.96   \n",
            "\n",
            "   Processed Billed Amount  \n",
            "0                  4665.40  \n",
            "1                   482.78  \n",
            "2                   762.92  \n",
            "3                  3125.01  \n",
            "4                   840.00  \n",
            "5                  1351.89  \n",
            "6                   694.19  \n",
            "7                    33.00  \n",
            "8                  2468.06  \n",
            "9                   109.96  \n",
            "\n",
            "--- Totals by Region ---\n",
            "     Region  Processed Billed Amount\n",
            "0  americas                 11922.19\n",
            "5     nemea                  7643.68\n",
            "1      dach                  6545.33\n",
            "2    france                  5281.13\n",
            "3       mea                  3400.73\n",
            "4     mixed                   206.80\n",
            "\n",
            "--- Script Finished ---\n"
          ]
        }
      ]
    }
  ]
}